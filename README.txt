*****************************************************************
*FingerBurner - Thwarting Plugin-based Browser Fingerprinting   *
*Houston Hunt, Alejandro Jove, Garrett Miller, Haley Nguyen     *
*94-806, Fall 2015, Carnegie Mellon University                  *
*****************************************************************

=========================================================
INSTALLING:
=========================================================

Under OS X (using Homebrew):
----------------------------
ruby -e "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)"
brew install python
pip install pyOpenSSL mitmproxy pyasn1
-----------------------------

Under Ubuntu 14.04 or newer:
----------------------------
sudo apt-get install python-pip python-dev libffi-dev libssl-dev libxml2-dev libxslt1-dev
sudo pip install pyOpenSSL mitmproxy pyasn1
-----------------------------

Test installation by running:
pydoc libmproxy.protocol.http.HTTPRequest

If you can view the document for the HTTPRequest class then you have installed
everything successfully.

=========================================================
RUNNING:
=========================================================
Run the proxy on your local computer with the detect.py (from repo)
mitmproxy -s detect.py

Visit a few webpages, and check the log file (generated by the script,
so read the script), and examine the logs to see which sites perform font
fingerprinting. We need a lot more work in this script, but this is a way to
get started.

Documentation on the proxy library:
https://mitmproxy.org/doc/scripting/inlinescripts.html

Install mitm certificate on browser to intercept HTTPS traffic:
mitmproxy generates its own self-signed cert the first time it runs in ~/.mitm/*

For Firefox on OS X, go to browser Preferences > Advanced > Network > Certificates 
and import mitmproxy-ca-cert.pem in the above folder in order to capture HTTP 
traffic.

For more info see https://mitmproxy.org/doc/certinstall.html

=========================================================
RUNNING WEB CRAWLER:
=========================================================
First, install scrapy with pip:
sudo pip install scrapy

Then in fingerburner/webcrawler, run

scrapy crawl webcrawler -a pattern=url_pattern.txt

Give it some time, and run Ctrl-C only once

The crawler will nicely clean up and leave a data.csv file in the working directory.
That file will contain a list of URLs of distinct domains.