*****************************************************************
*FingerBurner - Thwarting Plugin-based Browser Fingerprinting   *
*Houston Hunt, Alejandro Jove, Garrett Miller, Haley Nguyen     *
*94-806, Fall 2015, Carnegie Mellon University                  *
*****************************************************************

=========================================================
INSTALLING:
=========================================================
Install Python 2.7.9 or greater 2.7.X, if your system doesn't include it already.

Install pip for your OS:
http://pip.readthedocs.org/en/stable/installing/

Install pyOpenSSL, mitmproxy (and if you see InsecurePlatformWarning, try again)
sudo pip install pyOpenSSL mitmproxy

Test installation by running:
pydoc libmproxy.protocol.http.HTTPRequest

If you can view the document for HTTPRequest object then you have installed
everything successfully.

=========================================================
RUNNING:
=========================================================
Run the proxy on your local computer with the detect.py (from repo)
mitmproxy -s detect.py

Visit a few webpages, and check the log file (generated by the script,
so read the script), and examine the logs to see which sites perform font
fingerprinting. We need a lot more work in this script, but this is a way to
get started.

Documentation on the proxy library:
https://mitmproxy.org/doc/scripting/inlinescripts.html

Install mitm certificate on browser to intercept HTTPS traffic:
mitmproxy generates its own self-signed cert the first time it runs in ~/.mitm/*

For Firefox on OS X, go to browser Preferences > Advanced > Network > Certificates 
and import mitmproxy-ca-cert.pem in the above folder in order to capture HTTP 
traffic.

For more info see https://mitmproxy.org/doc/certinstall.html

=========================================================
RUNNING WEB CRAWLER:
=========================================================
First, install scrapy with pip:
sudo pip install scrapy

Then in fingerburner/webcrawler, run

scrapy crawl webcrawler -a pattern=url_pattern.txt

Give it some time, and run Ctrl-C only once

The crawler will nicely clean up and leave a data.csv file in the working directory.
That file will contain a list of URLs of distinct domains.